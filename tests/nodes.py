# nodes.py

from dotenv import load_dotenv
load_dotenv()

from typing import List, Tuple
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from state import State
from llm import get_llm
from db import get_vectorstore

# messages/stateì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
def _get_question(state: State) -> str:
    q = (state.get("user_question") or "").strip()
    if q:
        return q

    msgs = state.get("messages") or []
    for m in reversed(list(msgs)):
        content = None
        role = None

        if hasattr(m, "content"):
            content = getattr(m, "content", None)
            role = getattr(m, "type", None) or getattr(m, "role", None)

        if isinstance(m, dict):
            content = m.get("content", content)
            role = m.get("type") or m.get("role") or role

        if isinstance(content, str) and content.strip():
            if role in (None, "human", "user"):
                return content.strip()

    return ""

# 1) ì‚¬ìš©ì ì§ˆë¬¸ ì •ì œ
def refine_question(state: State) -> dict:
    llm = get_llm("gen")
    question = _get_question(state)

    prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)" HR ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë…¸ë“œì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì •ì œí•´ ì£¼ì„¸ìš”.
    ê·œì¹™:
    1. ì–¸ì–´ ê·œì¹™
     - ê¸°ë³¸ ì–¸ì–´ëŠ” í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤.
     - í•œêµ­ì–´ ë¬¸ë§¥ ì•ˆì— ìˆ«ìë‚˜ ì¼ë¶€ ì˜ì–´ ë‹¨ì–´(point, vacation ë“±)ê°€ ì„ì—¬ ìˆëŠ” ê²½ìš°ëŠ” í—ˆìš©í•©ë‹ˆë‹¤.
     - í•œêµ­ì–´ ì—†ì´ ì „ë¶€ ì˜ì–´ë¡œë§Œ ì…ë ¥ëœ ê²½ìš°ëŠ” "invalid_input"ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    2. í˜•ì‹ ì •ë¦¬
     - ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ìëŠ” ì œê±°í•©ë‹ˆë‹¤.
     - ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ëŠ” ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸(?, !, ., ,)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤.
     - ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
    3. í‘œí˜„ í‘œì¤€í™”
    ë¬¸ë§¥ì„ íŒŒì•…í•˜ì—¬ HR ìš©ì–´ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    í‘œì¤€í™” ì˜ˆì‹œ:
        - "ì‰¬ë ¤ê³  í•˜ëŠ”ë° í•˜ë£¨ì— ë°˜ë§Œ" â†’ "ë°˜ì°¨ ì•ˆë‚´"
        - "ì»´í“¨í„° ë¡œê·¸ì¸ì´ ì•ˆ ë¼" â†’ "ê³„ì • ë³´ì•ˆ ë¬¸ì œ"
        - "íšŒì‚¬ ë™í˜¸íšŒ ëˆ ì§€ì›í•´ì¤˜?" â†’ "ì‚¬ë‚´ ë™í˜¸íšŒ ì§€ì›"
        - "ì¶œê·¼ ì¢€ ëŠ¦ê²Œ í•´ë„ ë¼?" â†’ "ì‹œì°¨ ì¶œê·¼ ì œë„"
        - "ê¸‰ì—¬ì¼ì´ ì–¸ì œì•¼?" â†’ "ê¸‰ì—¬ì¼ ì•ˆë‚´"
        - "ë³µì§€ point ì–¼ë§ˆì§€? 1000í¬ì¸íŠ¸ì¸ê°€?" â†’ "ë³µì§€ í¬ì¸íŠ¸ ì•ˆë‚´"
        - "ë‚˜ ë°˜          ì°¨ ì“¸ ìˆ˜ ìˆì–´?" â†’ "ë°˜ì°¨ ì•ˆë‚´"
        ë™ì˜ì–´, ìœ ì˜ì–´, ì¤„ì„ë§, ì´ˆì„± í‘œí˜„ë„ í‘œì¤€í™” í•©ë‹ˆë‹¤.
        ì˜ˆì‹œ:
        - "ëŒ€íœ´" â†’ "ëŒ€ì²´íœ´ê°€"
        - "ã„±ã„±" â†’ "ê³ ê³ "
        - "ã…‡ã…‡" â†’ "ì‘ì‘"
        - "ë‚´ê·œ" â†’ "ë‚´ë¶€ê·œì¹™"

    ì‚¬ìš©ì ì§ˆë¬¸:
        {question}
    ìœ„ ê·œì¹™ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œê±°í•˜ê³  ì¶œë ¥í•˜ë¼.
    """.strip()

    result = llm.invoke(prompt).content.strip() if question else ""
    return {
        "user_question": question,
        "refined_question": result
    }


# 2) ë¬¸ì„œ ê²€ìƒ‰
def retrieve(state: State) -> dict:
    # ë¯¸ë¦¬ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì— ì—°ê²°í•˜ì—¬ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    # db.pyì˜ get_vectorstore í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    vs = get_vectorstore(index_name="gaida-hr-rules")
    
    refined_question = state.get("refined_question", "") or _get_question(state) or ""
    if not refined_question:
        # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {"retrieved_docs": []}
        
    # retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ë„ ë†’ì€ ë¬¸ì„œë¥¼ 3ê°œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    retriever = vs.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(refined_question)
    
    return {"retrieved_docs": docs}

# 3) ì¬ìˆœìœ„í™”(ì •ê·œì‹ ê¸°ë°˜ íŒŒì‹± ìœ ì§€)
def rerank(state: State) -> dict:
    llm = get_llm("gen")
    question = _get_question(state)
    if not question or not state.get("retrieved_docs"):
        return {"retrieved_docs": state.get("retrieved_docs", [])}

    import re
    scored: List[Tuple[Document, float]] = []

    for doc in state.get("retrieved_docs", []):
        prompt = f"""
        ì§ˆë¬¸: "{question}"
        ë¬¸ì„œ ë‚´ìš©: "{doc.page_content}"
        0~1 ì‚¬ì´ ìˆ«ìë¡œ ê´€ë ¨ë„ë§Œ ì¶œë ¥:
        """.strip()
        txt = (llm.invoke(prompt).content or "").strip()
        cleaned = txt.replace(",", ".")
        m = re.search(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", cleaned)
        try:
            score = float(m.group()) if m else 0.0
        except Exception:
            score = 0.0
        score = max(0.0, min(1.0, score))
        scored.append((doc, score))

    scored.sort(key=lambda x: x[1], reverse=True)
    top_docs = [doc for doc, _ in scored[:3]]
    return {"retrieved_docs": top_docs}

# 4) ë‹µë³€ ìƒì„±
def generate_rag_answer(state: State) -> dict:
    llm = get_llm("gen")
    question = _get_question(state)
    if not question:
        return {"final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}

    context = ""
    for i, doc in enumerate(state.get("retrieved_docs", []), start=1):
        context += f"[{i}] ({doc.metadata.get('source', 'unknown')})\n{doc.page_content}\n\n"

    if not context.strip():
        return {"final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê´€ë ¨ ì¶œì²˜ê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)"ì˜ ì¹œì ˆí•œ HR ì •ì±… ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì•„ë˜ ì¶œì²˜ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ í•´ì„œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    ë‹µë³€ ë³¸ë¬¸ ì¤‘ ì¸ìš©í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´, ë¬¸ì¥ ëì— [ì¶œì²˜ ë²ˆí˜¸]ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.
    ë‹µë³€ì˜ ë§ˆì§€ë§‰ì—ëŠ” 'ì¶œì²˜ ëª©ë¡'ì„ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.

    # ì§ˆë¬¸
    {question}

    # ì¶œì²˜ ë¬¸ì„œ
    {context}

    # ë‹µë³€
    """.strip()

    answer = llm.invoke(prompt).content.strip()
    return {
        "messages": [AIMessage(content=answer)],
        "final_answer": answer
    }

# 5) ë‹µë³€ ê²€ì¦
def verify_rag_answer(state: State) -> dict:
    llm = get_llm("gen")

    # [ìˆ˜ì •] ê²€ì¦ì„ ìœ„í•´ ë¬¸ì„œì˜ 'ì´ë¦„'ì´ ì•„ë‹Œ 'ë‚´ìš©'ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    context = ""
    for doc in state.get("retrieved_docs", []):
        context += f"- {doc.page_content}\n"

    final_answer = state.get("final_answer", "")

    # [ì¶”ê°€] ì»¨í…ìŠ¤íŠ¸ë‚˜ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê²€ì¦ì´ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ 'ë¶ˆì¼ì¹˜í•¨'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if not context.strip() or not final_answer.strip():
        return {"verification": "ë¶ˆì¼ì¹˜í•¨"}

    prompt = f"""
    ë‹¹ì‹ ì€ ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê·¼ê±°í–ˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” AI í‰ê°€ìì…ë‹ˆë‹¤.
    'ë‹µë³€'ì´ ì•„ë˜ 'ë¬¸ì„œ' ë‚´ìš©ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ 'ì¼ì¹˜í•¨'ì„, ì¡°ê¸ˆì´ë¼ë„ ë‹¤ë¥´ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ 'ë¶ˆì¼ì¹˜í•¨'ì„ ì¶œë ¥í•˜ì„¸ìš”.
    ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì¶”ê°€í•˜ì§€ ë§ê³ , 'ì¼ì¹˜í•¨' ë˜ëŠ” 'ë¶ˆì¼ì¹˜í•¨' ë‘ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

    # ë¬¸ì„œ
    {context}

    # ë‹µë³€
    "{final_answer}"

    # íŒë‹¨ (ì¼ì¹˜í•¨/ë¶ˆì¼ì¹˜í•¨):
    """.strip()

    verdict = llm.invoke(prompt).content.strip()

    # [ê°œì„ ] LLMì´ ì§€ì‹œë¥¼ ì–´ê¸°ê³  "ë„¤, ì¼ì¹˜í•©ë‹ˆë‹¤."ì™€ ê°™ì´ ë‹µë³€í•´ë„ ì²˜ë¦¬ ê°€ëŠ¥
    if "ì¼ì¹˜í•¨" in verdict:
        final_verdict = "ì¼ì¹˜í•¨"
    else:
        final_verdict = "ë¶ˆì¼ì¹˜í•¨"
    return {"verification": final_verdict}

# 6) ë‹´ë‹¹ì ì•ˆë‚´ ë‹µë³€ ìƒì„±
def generate_contact_answer(state: State) -> dict:
    """
    ë‹´ë‹¹ì ì•ˆë‚´ ì‘ë‹µ ìƒì„±
    """
    department = state.get('department_info') 

    if not department:
        # ê¸°ë³¸ê°’: ì¸ì‚¬íŒ€
        department = {"name": "ì¸ì‚¬", "email": "hr@gaida.play.com", "slack": "#ask-hr"}
    
    response = f"""
í•´ë‹¹ ë¬¸ì˜ì‚¬í•­ì€ **{department['name']}íŒ€**ìœ¼ë¡œ ë¬¸ì˜í•˜ì‹œë©´ ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“§ **ì´ë©”ì¼**: {department['email']}
ğŸ’¬ **ìŠ¬ë™**: {department['slack']}

ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š
    """.strip()
    
    return {
        "messages": [AIMessage(content=response)],
        "final_answer": response
    }