# nodes.py

from dotenv import load_dotenv
load_dotenv()

import json
from typing import Dict, List, Tuple, Optional, TypedDict, Literal, cast
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from state import State
from utils import get_llm
from scripts.create_pinecone_index import get_vectorstore


# =============================================
# Node: ì‚¬ìš©ì ì§ˆë¬¸ ì •ì œ
# =============================================
 
def _get_question(state: State) -> str:
    """
    messages/stateì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    """
    q = (state.get("user_question") or "").strip()
    if q:
        return q

    msgs = state.get("messages") or []
    for m in reversed(list(msgs)):
        content = None
        role = None

        if hasattr(m, "content"):
            content = getattr(m, "content", None)
            role = getattr(m, "type", None) or getattr(m, "role", None)

        if isinstance(m, dict):
            content = m.get("content", content)
            role = m.get("type") or m.get("role") or role

        if isinstance(content, str) and content.strip():
            if role in (None, "human", "user"):
                return content.strip()

    return ""


def refine_question(state: State) -> dict:
    _llm = get_llm("gen")
    question = _get_question(state)

    prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)" HR ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë…¸ë“œì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì •ì œí•´ ì£¼ì„¸ìš”.
    ê·œì¹™:
    1. ì–¸ì–´ ê·œì¹™
     - ê¸°ë³¸ ì–¸ì–´ëŠ” í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤.
     - í•œêµ­ì–´ ë¬¸ë§¥ ì•ˆì— ìˆ«ìë‚˜ ì¼ë¶€ ì˜ì–´ ë‹¨ì–´(point, vacation ë“±)ê°€ ì„ì—¬ ìˆëŠ” ê²½ìš°ëŠ” í—ˆìš©í•©ë‹ˆë‹¤.
     - í•œêµ­ì–´ ì—†ì´ ì „ë¶€ ì˜ì–´ë¡œë§Œ ì…ë ¥ëœ ê²½ìš°ëŠ” "invalid_input"ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    2. í˜•ì‹ ì •ë¦¬
     - ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ìëŠ” ì œê±°í•©ë‹ˆë‹¤.
     - ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ëŠ” ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸(?, !, ., ,)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤.
     - ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
    3. í‘œí˜„ í‘œì¤€í™”
    ë¬¸ë§¥ì„ íŒŒì•…í•˜ì—¬ HR ìš©ì–´ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    í‘œì¤€í™” ì˜ˆì‹œ:
        - "ì‰¬ë ¤ê³  í•˜ëŠ”ë° í•˜ë£¨ì— ë°˜ë§Œ" â†’ "ë°˜ì°¨ ì•ˆë‚´"
        - "ì»´í“¨í„° ë¡œê·¸ì¸ì´ ì•ˆ ë¼" â†’ "ê³„ì • ë³´ì•ˆ ë¬¸ì œ"
        - "íšŒì‚¬ ë™í˜¸íšŒ ëˆ ì§€ì›í•´ì¤˜?" â†’ "ì‚¬ë‚´ ë™í˜¸íšŒ ì§€ì›"
        - "ì¶œê·¼ ì¢€ ëŠ¦ê²Œ í•´ë„ ë¼?" â†’ "ì‹œì°¨ ì¶œê·¼ ì œë„"
        - "ê¸‰ì—¬ì¼ì´ ì–¸ì œì•¼?" â†’ "ê¸‰ì—¬ì¼ ì•ˆë‚´"
        - "ë³µì§€ point ì–¼ë§ˆì§€? 1000í¬ì¸íŠ¸ì¸ê°€?" â†’ "ë³µì§€ í¬ì¸íŠ¸ ì•ˆë‚´"
        - "ë‚˜ ë°˜          ì°¨ ì“¸ ìˆ˜ ìˆì–´?" â†’ "ë°˜ì°¨ ì•ˆë‚´"
        ë™ì˜ì–´, ìœ ì˜ì–´, ì¤„ì„ë§, ì´ˆì„± í‘œí˜„ë„ í‘œì¤€í™” í•©ë‹ˆë‹¤.
        ì˜ˆì‹œ:
        - "ëŒ€íœ´" â†’ "ëŒ€ì²´íœ´ê°€"
        - "ã„±ã„±" â†’ "ê³ ê³ "
        - "ã…‡ã…‡" â†’ "ì‘ì‘"
        - "ë‚´ê·œ" â†’ "ë‚´ë¶€ê·œì¹™"

    ì‚¬ìš©ì ì§ˆë¬¸:
        {question}
    ìœ„ ê·œì¹™ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œê±°í•˜ê³  ì¶œë ¥í•˜ë¼.
    """.strip()

    result = _llm.invoke(prompt).content.strip() if question else ""
    return {
        "user_question": question,
        "refined_question": result
    }


# =============================================
# Node: ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
# =============================================

def retrieve(state: State) -> dict:
    # ë¯¸ë¦¬ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì— ì—°ê²°í•˜ì—¬ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    # db.pyì˜ get_vectorstore í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    vs = get_vectorstore(index_name="gaida-hr-rules")
    
    refined_question = state.get("refined_question", "") or _get_question(state) or ""
    if not refined_question:
        # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {"retrieved_docs": []}
        
    # retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ë„ ë†’ì€ ë¬¸ì„œë¥¼ 3ê°œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    retriever = vs.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(refined_question)
    
    return {"retrieved_docs": docs}


# =============================================
# Node: ì¬ìˆœìœ„í™”(ì •ê·œì‹ ê¸°ë°˜ íŒŒì‹± ìœ ì§€)
# =============================================

def rerank(state: State) -> dict:
    llm = get_llm("gen")
    question = _get_question(state)
    if not question or not state.get("retrieved_docs"):
        return {"retrieved_docs": state.get("retrieved_docs", [])}

    import re
    scored: List[Tuple[Document, float]] = []

    for doc in state.get("retrieved_docs", []):
        prompt = f"""
        ì§ˆë¬¸: "{question}"
        ë¬¸ì„œ ë‚´ìš©: "{doc.page_content}"
        0~1 ì‚¬ì´ ìˆ«ìë¡œ ê´€ë ¨ë„ë§Œ ì¶œë ¥:
        """.strip()
        txt = (llm.invoke(prompt).content or "").strip()
        cleaned = txt.replace(",", ".")
        m = re.search(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", cleaned)
        try:
            score = float(m.group()) if m else 0.0
        except Exception:
            score = 0.0
        score = max(0.0, min(1.0, score))
        scored.append((doc, score))

    scored.sort(key=lambda x: x[1], reverse=True)
    top_docs = [doc for doc, _ in scored[:3]]
    return {"retrieved_docs": top_docs}


# =========================
# Answer_type: Rag_answer
# =========================

def generate_rag_answer(state: State) -> dict:
    _llm = get_llm("gen")
    question = _get_question(state)
    if not question:
        return {"final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}

    context = ""
    for i, doc in enumerate(state.get("retrieved_docs", []), start=1):
        context += f"[{i}] ({doc.metadata.get('source', 'unknown')})\n{doc.page_content}\n\n"

    if not context.strip():
        return {"final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê´€ë ¨ ì¶œì²˜ê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)"ì˜ ì¹œì ˆí•œ HR ì •ì±… ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì•„ë˜ ì¶œì²˜ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ í•´ì„œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    ë‹µë³€ ë³¸ë¬¸ ì¤‘ ì¸ìš©í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´, ë¬¸ì¥ ëì— [ì¶œì²˜ ë²ˆí˜¸]ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.
    ë‹µë³€ì˜ ë§ˆì§€ë§‰ì—ëŠ” 'ì¶œì²˜ ëª©ë¡'ì„ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.

    # ì§ˆë¬¸
    {question}

    # ì¶œì²˜ ë¬¸ì„œ
    {context}

    # ë‹µë³€
    """.strip()

    answer = _llm.invoke(prompt).content.strip()
    return {
        "messages": [AIMessage(content=answer)],
        "final_answer": answer
    }


# =============================================
# Node: RAG ë‹µë³€ ê²€ì¦
# =============================================

def verify_rag_answer(state: State) -> dict:
    _llm = get_llm("gen")

    # [ìˆ˜ì •] ê²€ì¦ì„ ìœ„í•´ ë¬¸ì„œì˜ 'ì´ë¦„'ì´ ì•„ë‹Œ 'ë‚´ìš©'ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    context = ""
    for doc in state.get("retrieved_docs", []):
        context += f"- {doc.page_content}\n"

    final_answer = state.get("final_answer", "")

    # [ì¶”ê°€] ì»¨í…ìŠ¤íŠ¸ë‚˜ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê²€ì¦ì´ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ 'ë¶ˆì¼ì¹˜í•¨'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if not context.strip() or not final_answer.strip():
        return {"verification": "ë¶ˆì¼ì¹˜í•¨"}

    prompt = f"""
    ë‹¹ì‹ ì€ ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê·¼ê±°í–ˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” AI í‰ê°€ìì…ë‹ˆë‹¤.
    'ë‹µë³€'ì´ ì•„ë˜ 'ë¬¸ì„œ' ë‚´ìš©ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ 'ì¼ì¹˜í•¨'ì„, ì¡°ê¸ˆì´ë¼ë„ ë‹¤ë¥´ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ 'ë¶ˆì¼ì¹˜í•¨'ì„ ì¶œë ¥í•˜ì„¸ìš”.
    ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì¶”ê°€í•˜ì§€ ë§ê³ , 'ì¼ì¹˜í•¨' ë˜ëŠ” 'ë¶ˆì¼ì¹˜í•¨' ë‘ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

    # ë¬¸ì„œ
    {context}

    # ë‹µë³€
    "{final_answer}"

    # íŒë‹¨ (ì¼ì¹˜í•¨/ë¶ˆì¼ì¹˜í•¨):
    """.strip()

    verdict = _llm.invoke(prompt).content.strip()

    # [ê°œì„ ] LLMì´ ì§€ì‹œë¥¼ ì–´ê¸°ê³  "ë„¤, ì¼ì¹˜í•©ë‹ˆë‹¤."ì™€ ê°™ì´ ë‹µë³€í•´ë„ ì²˜ë¦¬ ê°€ëŠ¥
    if "ì¼ì¹˜í•¨" in verdict:
        final_verdict = "ì¼ì¹˜í•¨"
    else:
        final_verdict = "ë¶ˆì¼ì¹˜í•¨"
    return {"verification": final_verdict}




# =============================================
# Node: HR ì—¬ë¶€ íŒë³„
# =============================================

class HRAnalysis(TypedDict):
    is_hr_question: bool

# ë¶€ì„œ ì •ë³´ dict (ì´ë©”ì¼ê³¼ ìŠ¬ë™ ì±„ë„)
DEPARTMENTS = {
    "ì¬ë¬´": {"name": "ì¬ë¬´", "email": "fi@gaida.play.com", "slack": "#ask-fi"},
    "ì´ë¬´": {"name": "ì´ë¬´", "email": "ga@gaida.play.com", "slack": "#ask-ga"},
    "ì¸í”„ë¼": {"name": "ì¸í”„ë¼", "email": "in@gaida.play.com", "slack": "#ask-in"},
    "ë³´ì•ˆ": {"name": "ë³´ì•ˆ", "email": "se@gaida.play.com", "slack": "#ask-se"},
    "ì¸ì‚¬": {"name": "ì¸ì‚¬", "email": "hr@gaida.play.com", "slack": "#ask-hr"},
}

def update_hr_status(state: State) -> State:
    """
    HR ì—¬ë¶€ë§Œ íŒë³„, ê·¸ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
    """
    prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)"ì˜ HR ì •ì±… ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì›ë³¸ ì§ˆë¬¸ì„ ì°¸ê³ í•´ì„œ ì •ì œ ì§ˆë¬¸ì´ HR ê´€ë ¨ì¸ì§€ íŒë³„í•˜ì„¸ìš”.

    ì›ë³¸ ì§ˆë¬¸: "{state['user_question']}"
    ì •ì œ ì§ˆë¬¸: "{state['refined_question']}"

    # ë¶„ë¥˜ ê¸°ì¤€
    ## HRê³¼ ê´€ë ¨ ì—†ëŠ” ê²½ìš°
    - ê°œì¸ì •ë³´ (ì˜ˆ: ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì´ë¦„)
    - íšŒì‚¬ ë‚´ë¶€ ë³´ì•ˆ ë‚´ìš© (ì˜ˆ: íšŒì‚¬ ì¬ì • ìƒí™©, ì‹ ê·œ í”„ë¡œì íŠ¸, íšŒì‚¬ì˜ ì¤‘ìš”í•œ ë‚´ë¶€ ë¬¸ê±´)
    - ë²•ë¥  ìë¬¸ ìš”ì²­ì´ë‚˜ ë²•ë¥  ìƒë‹´ í†¤ì˜ ì§ˆë¬¸

    ## HRê³¼ ê´€ë ¨ ìˆëŠ” ê²½ìš°
    - HR(ì¸ì‚¬/ê·¼ë¬´/íœ´ê°€/ë³µì§€/ì¥ë¹„Â·ë³´ì•ˆ/ì¶œì¥Â·ë¹„ìš©ì²˜ë¦¬ ë“±)

    # ì‘ë‹µ í˜•ì‹
    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

    HRê³¼ ê´€ë ¨ì—†ëŠ” ê²½ìš°:
    {{"is_hr_question": false}}

    HRê³¼ ê´€ë ¨ìˆëŠ” ê²½ìš°:
    {{"is_hr_question": true}}
    """
    _llm = get_llm("router1")
    structured_llm = _llm.with_structured_output(HRAnalysis)

    result: HRAnalysis = structured_llm.invoke(prompt)
    is_hr = result["is_hr_question"]

    # HR ì—¬ë¶€ì— ë”°ë¼ answer_type ì„¸íŒ…
    answer_type = "pending" if is_hr else "reject"

    return cast(State, {**state, "is_hr_question": is_hr, "answer_type": answer_type})


# =========================
# Answer_type: Reject
# =========================

def generate_reject_answer(state: State):
    """HR ê´€ë ¨ì´ ì•„ë‹Œ ì§ˆë¬¸ì— ëŒ€í•œ ê±°ë¶€ ë©”ì‹œì§€"""
    # return "ì…ë ¥í•˜ì‹  ì§ˆë¬¸ì€ HR ê´€ë ¨ ë¬¸ì˜ê°€ ì•„ë‹™ë‹ˆë‹¤. HR ê´€ë ¨ ì§ˆë¬¸ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    reject_answer = "ì…ë ¥í•˜ì‹  ì§ˆë¬¸ì€ HR ê´€ë ¨ ë¬¸ì˜ê°€ ì•„ë‹™ë‹ˆë‹¤. HR ê´€ë ¨ ì§ˆë¬¸ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    return {
        "messages": [AIMessage(content=reject_answer)],
        "final_answer": reject_answer
    }




# =============================================
# Node: RAG ì—¬ë¶€ íŒë³„
# =============================================

class RAGDepartmentAnalysis(TypedDict, total=False):
    route: str  # "rag" ë˜ëŠ” "department"
    department: str  # departmentì¸ ê²½ìš°ì—ë§Œ ê°’ì´ ìˆìŒ

def _classify_rag_or_department(question: str) -> Dict[str, str]:
    """LLMì„ ì‚¬ìš©í•œ í†µí•© ë¶„ë¥˜: RAG vs ë‹´ë‹¹ì ì•ˆë‚´ + ë¶€ì„œ ê²°ì •"""
    
    system_prompt = f"""
    ë‹¹ì‹ ì€ "ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS)" HR ì±—ë´‡ì˜ ì§ˆë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì •ì œëœ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ê²°ì •í•´ì£¼ì„¸ìš”.

    ì •ì œëœ ì§ˆë¬¸: "{question}"

    # ë¶„ë¥˜ ê¸°ì¤€

    ## 1. RAG ì²˜ë¦¬ ëŒ€ìƒ (route: "rag")
    - íšŒì‚¬ ë‚´ë¶€ ê·œì •, ì •ì±…, ì œë„ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§ˆë¬¸
    - ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ì„± ì§ˆë¬¸
    - ì˜ˆì‹œ:
    * "ì—°ì°¨ ê·œì •ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    * "ì¬íƒê·¼ë¬´ ì •ì±…ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    * "ë³µì§€ì œë„ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
    * "ê·¼ë¬´ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    * "íœ´ê°€ ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    * "ì¥ë¹„ ì‚¬ìš© ê·œì¹™ì´ ê¶ê¸ˆí•´ìš”"

    ## 2. ë‹´ë‹¹ì ì•ˆë‚´ ëŒ€ìƒ (route: "department")
    - ê°œì¸ë³„ ë§ì¶¤ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì§ˆë¬¸
    - ì‹¤ì‹œê°„ ì²˜ë¦¬ë‚˜ ìŠ¹ì¸ì´ í•„ìš”í•œ ì—…ë¬´
    - ë¬¸ì œ í•´ê²°ì´ë‚˜ ì‹ ê³ ê°€ í•„ìš”í•œ ìƒí™©
    - ê°œë³„ ìƒë‹´ì´ í•„ìš”í•œ ë¯¼ê°í•œ ì‚¬ì•ˆ

    ### ë¶€ì„œë³„ ë‹´ë‹¹ ì—…ë¬´:
    - **ì¬ë¬´**: ì„¸ê¸ˆ, ì˜ˆì‚°, íšŒê³„, ì§€ì¶œ, ì†¡ê¸ˆ, ê³„ì‚°ì„œ, ì²­êµ¬ì„œ, ì§€ê¸‰, ë¹„ìš©, í™˜ê¸‰
    - **ì´ë¬´**: ì‚¬ë¬´ì‹¤, ë¹„í’ˆ, ë¬¼í’ˆ, êµ¬ë§¤, ìˆ˜ë ¹, ìš°í¸, ì‚¬ë¬´ìš©í’ˆ, ì‹œì„¤, í–‰ì‚¬, ì°¨ëŸ‰, ì²­ì†Œ, ìì‚°, ì¶œì¥, ìˆ™ë°•, êµí†µ
    - **ì¸í”„ë¼**: ì„œë²„, ë„¤íŠ¸ì›Œí¬, ì»´í“¨í„°, IT, ì†Œí”„íŠ¸ì›¨ì–´, ì¥ë¹„, ì‹œìŠ¤í…œ, ì ‘ì†, VPN, ê³„ì •, ì ‘ê·¼
    - **ë³´ì•ˆ**: ë³´ì•ˆ, í•´í‚¹, ì •ë³´, ìœ ì¶œ, ì¹¨í•´, ëœì„¬ì›¨ì–´, ë°±ì‹ , ë°ì´í„°, ë¹„ë°€ë²ˆí˜¸, ë°©í™”ë²½, ì•…ì„±ì½”ë“œ, ì•”í˜¸
    - **ì¸ì‚¬**: ê°œë³„ ê¸‰ì—¬ ë¬¸ì˜, ì±„ìš©, ì¸ì‚¬í‰ê°€, í‡´ì§, í‡´ì§ê¸ˆ ê³„ì‚° ë° ì§€ê¸‰, ì…ì‚¬, í‡´ì‚¬, í‰ê°€, ìŠ¹ì§„, ê°œì¸ì  ê·¼ë¬´ ìƒë‹´

    # ì‘ë‹µ í˜•ì‹
    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

    RAG ì²˜ë¦¬ì¸ ê²½ìš°:
    {{"route": "rag"}}

    ë‹´ë‹¹ì ì•ˆë‚´ì¸ ê²½ìš°:
    {{"route": "department", "department": "ë¶€ì„œëª…"}}

    ë¶€ì„œëª…ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¬ë¬´, ì´ë¬´, ì¸í”„ë¼, ë³´ì•ˆ, ì¸ì‚¬

    ë¶€ë“ì´í•˜ê²Œ ì¬ë¬´, ì´ë¬´, ì¸í”„ë¼, ë³´ì•ˆ ë¶€ì„œì— í•´ë‹¹í•˜ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” ì¸ì‚¬ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.
    """

    _llm = get_llm("router2")
    structured_llm = _llm.with_structured_output(RAGDepartmentAnalysis)
    
    try:
        result: RAGDepartmentAnalysis = structured_llm.invoke(system_prompt)
        return result
        
    except Exception as e:
        print(f"LLM ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ê°’: ì¸ì‚¬íŒ€ ë‹´ë‹¹ì ì•ˆë‚´ë¡œ ë¼ìš°íŒ…
        return {"route": "department", "department": "ì¸ì‚¬"}

def update_rag_status(state: State) -> State:
    """LLM ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜ ë° ë¼ìš°íŒ… ìƒíƒœ ì—…ë°ì´íŠ¸"""
    question = state['refined_question']
    
    print(f" LLM ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜ ì‹œì‘...")
    
    # LLMì„ í†µí•œ í†µí•© ë¶„ë¥˜
    classification_result = _classify_rag_or_department(question)
    
    route = classification_result.get("route")
    department_name = classification_result.get("department")
    
    print(f" ë¶„ë¥˜ ê²°ê³¼: {classification_result}")
    
    if route == "rag":
        # RAG ì²˜ë¦¬ë¡œ ë¶„ë¥˜
        print("â¡ï¸ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë¼ìš°íŒ…")
        return cast(State, {**state, "is_rag_suitable": True, "department_info": None, "answer_type": "rag_answer"})
    else:
        # ë‹´ë‹¹ì ì•ˆë‚´ë¡œ ë¶„ë¥˜
        department_info = DEPARTMENTS.get(department_name, DEPARTMENTS["ì¸ì‚¬"])
        print(f"â¡ï¸ {department_name}íŒ€ ë‹´ë‹¹ì ì•ˆë‚´ë¡œ ë¼ìš°íŒ…")
        return cast(State, {**state, "is_rag_suitable": False, "department_info": department_info, "answer_type": "department_contact"})


# =========================
# Answer_type: Department_contact
# =========================

def generate_contact_answer(state: State) -> dict:
    """
    ë‹´ë‹¹ì ì•ˆë‚´ ì‘ë‹µ ìƒì„±
    """
    department = state.get('department_info') 

    if not department:
        # ê¸°ë³¸ê°’: ì¸ì‚¬íŒ€
        department = {"name": "ì¸ì‚¬", "email": "hr@gaida.play.com", "slack": "#ask-hr"}
    
    response = f"""
í•´ë‹¹ ë¬¸ì˜ì‚¬í•­ì€ **{department['name']}íŒ€**ìœ¼ë¡œ ë¬¸ì˜í•˜ì‹œë©´ ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“§ **ì´ë©”ì¼**: {department['email']}
ğŸ’¬ **ìŠ¬ë™**: {department['slack']}

ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š
    """.strip()
    
    return {
        "messages": [AIMessage(content=response)],
        "final_answer": response
    }