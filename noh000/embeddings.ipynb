{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4c9dcc",
   "metadata": {},
   "source": [
    "# ë¡œë” í…ŒìŠ¤íŠ¸\n",
    "- 04_ë³µì§€ì •ì±…_v1.0.md ê¸°ì¤€  \n",
    "TextLoader: 0.0285ì´ˆ  \n",
    "UnstructuredMarkdownLoader: 18.1923ì´ˆ  \n",
    "ì„±ëŠ¥ ì°¨ì´: 637.5ë°°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "def performance_test(file_path: str):\n",
    "    # TextLoader í…ŒìŠ¤íŠ¸\n",
    "    start = time.time()\n",
    "    text_loader = TextLoader(file_path, encoding=\"UTF-8\")\n",
    "    text_docs = text_loader.load()\n",
    "    text_time = time.time() - start\n",
    "    print(f\"TextLoader: {text_time:.4f}ì´ˆ\")\n",
    "    \n",
    "    # UnstructuredMarkdownLoader í…ŒìŠ¤íŠ¸\n",
    "    start = time.time()\n",
    "    unstructured_loader = UnstructuredMarkdownLoader(\n",
    "        file_path, \n",
    "        mode=\"elements\",\n",
    "        strategy=\"fast\"\n",
    "    )\n",
    "    unstructured_docs = unstructured_loader.load()\n",
    "    unstructured_time = time.time() - start\n",
    "    print(f\"UnstructuredMarkdownLoader: {unstructured_time:.4f}ì´ˆ\")\n",
    "    \n",
    "    print(f\"ì„±ëŠ¥ ì°¨ì´: {unstructured_time/text_time:.1f}ë°°\")\n",
    "\n",
    "file_path = \"./04_ë³µì§€ì •ì±…_v1.0.md\"\n",
    "performance_test(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580611ac",
   "metadata": {},
   "source": [
    "# TextLoader + í—¤ë” ê¸°ë°˜ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_hr_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    HR ì •ì±… ë¬¸ì„œë¥¼ TextLoader + MarkdownHeaderTextSplitterë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë“¤\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TextLoaderë¡œ ë¬¸ì„œ ë¡œë“œ\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 2. Document ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ\n",
    "    document_text = documents[0].page_content\n",
    "    \n",
    "    # 3. HR ë¬¸ì„œ êµ¬ì¡°ì— ë§ëŠ” í—¤ë” ì •ì˜\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"ë¬¸ì„œì œëª©\"),          # # ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ\n",
    "        (\"##\", \"ì •ì±…ëŒ€ë¶„ë¥˜\"),       # ## 1. íœ´ê°€ ë° íœ´ì§ ì œë„\n",
    "        (\"###\", \"ì •ì±…ì„¸ë¶€í•­ëª©\"),    # ### 1.1 ì—°ì°¨íœ´ê°€\n",
    "        # (\"####\", \"ì„¸ë¶€ì ˆì°¨\"),       # #### **ì‹ ì²­ ì ˆì°¨**\n",
    "    ]\n",
    "    \n",
    "    # 4. MarkdownHeaderTextSplitterë¡œ êµ¬ì¡°ì  ë¶„í• \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # í—¤ë” ì •ë³´ ìœ ì§€ (ì»¨í…ìŠ¤íŠ¸ì— ì¤‘ìš”)\n",
    "    )\n",
    "    \n",
    "    # 5. ë¬¸ìì—´ì„ split_textì— ì „ë‹¬ (Documentê°€ ì•„ë‹Œ str)\n",
    "    md_header_splits = markdown_splitter.split_text(document_text)\n",
    "    \n",
    "    # 6. ê¸´ ì„¹ì…˜ì„ ìœ„í•œ ì¶”ê°€ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # HR Q&Aì— ì í•©í•œ í¬ê¸°\n",
    "        chunk_overlap=200,      # ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„ë©\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # 7. ìµœì¢… ë¶„í•  ì ìš©\n",
    "    final_splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return final_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38200590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³µì§€ì •ì±… ì ìš©\n",
    "file_path = \"./04_ë³µì§€ì •ì±…_v1.0.md\"\n",
    "splits = load_hr_document(file_path)\n",
    "\n",
    "len_splits = len(splits)\n",
    "print(f\"âœ… ë¡œë”© ì„±ê³µ! ì´ {len_splits}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨\")\n",
    "print(\"=\"*100)\n",
    "for i in range(len_splits):\n",
    "    print(f\"ë©”íƒ€ë°ì´í„°: {splits[i].metadata}\")\n",
    "    print(f\"ë‚´ìš©: {splits[i].page_content[:200]}...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677f4b8",
   "metadata": {},
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ë¹„êµ\n",
    "- OpenAI text-embedding-3-small\n",
    "- BAAI/bge-m3\n",
    "- intfloat/multilingual-e5-large-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d847ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì¹˜ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    import openai\n",
    "    import sentence_transformers\n",
    "    import transformers\n",
    "    import faiss\n",
    "    import torch\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "    print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "def get_hr_test_queries() -> List[str]:\n",
    "    \"\"\"\n",
    "    HR ë¬¸ì„œ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    return [\n",
    "        \"ì—°ì°¨íœ´ê°€ëŠ” ëª‡ ì¼ì¸ê°€ìš”?\",\n",
    "        \"ë³µì§€í¬ì¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?\",\n",
    "        \"êµìœ¡ë¹„ ì§€ì› ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "        \"ìœ¡ì•„ ì§€ì›ê¸ˆì€ ì–¼ë§ˆë‚˜ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "        \"ê±´ê°•ê²€ì§„ì€ ì–¸ì œë¶€í„° ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "        \"ì¥ê¸° ê·¼ì†ì ê°€ì‚°ì¼ ê·œì •\",\n",
    "        \"ë³‘ê°€ ì‚¬ìš© ì‹œ í•„ìš”í•œ ì„œë¥˜\",\n",
    "        \"ë™ì•„ë¦¬ í™œë™ë¹„ ì§€ì› í•œë„\"\n",
    "    ]\n",
    "\n",
    "def test_search_performance(vector_store: FAISS, model_name: str, test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ì˜ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ” {model_name} ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_results = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nì§ˆë¬¸ {i}: {query}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ê²°ê³¼)\n",
    "        search_results = vector_store.similarity_search(query, k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        total_time += search_time\n",
    "        \n",
    "        print(f\"â±ï¸  ê²€ìƒ‰ ì‹œê°„: {search_time:.3f}ì´ˆ\")\n",
    "        print(\"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "        \n",
    "        query_result = {\n",
    "            \"query\": query,\n",
    "            \"search_time\": search_time,\n",
    "            \"results\": []\n",
    "        }\n",
    "        \n",
    "        for j, doc in enumerate(search_results, 1):\n",
    "            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)\n",
    "            preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "            print(f\"  {j}. {preview}...\")\n",
    "            \n",
    "            query_result[\"results\"].append({\n",
    "                \"rank\": j,\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        \n",
    "        model_results.append(query_result)\n",
    "    \n",
    "    result = {\n",
    "        \"model_name\": model_name,\n",
    "        \"total_time\": total_time,\n",
    "        \"avg_time\": total_time / len(test_queries),\n",
    "        \"queries\": model_results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} ì „ì²´ ì„±ëŠ¥:\")\n",
    "    print(f\"  ì´ ê²€ìƒ‰ ì‹œê°„: {total_time:.3f}ì´ˆ\")\n",
    "    print(f\"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {total_time/len(test_queries):.3f}ì´ˆ\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_openai_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    OpenAI text-embedding-3-small í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"OpenAI text-embedding-3-small\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        print(\"âœ… OpenAI ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def test_bge_m3_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    BAAI/bge-m3 í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-m3\",\n",
    "            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš©ì‹œ 'cuda'ë¡œ ë³€ê²½\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(\"âœ… BGE-M3 ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def test_e5_large_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    intfloat/multilingual-e5-large-instruct í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"multilingual-e5-large-instruct\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš©ì‹œ 'cuda'ë¡œ ë³€ê²½\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(\"âœ… E5-Large ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def compare_all_models(documents: List[Document]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ ë¹„êµ\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ ì„ë² ë”© ëª¨ë¸ ìˆœì°¨ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_queries = get_hr_test_queries()\n",
    "    results = []\n",
    "    \n",
    "    # ê° ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    print(f\"\\nğŸ“‹ {len(test_queries)}ê°œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    \n",
    "    # 1. OpenAI í…ŒìŠ¤íŠ¸\n",
    "    openai_result = test_openai_embedding(documents, test_queries)\n",
    "    results.append(openai_result)\n",
    "    \n",
    "    # 2. BGE-M3 í…ŒìŠ¤íŠ¸  \n",
    "    bge_result = test_bge_m3_embedding(documents, test_queries)\n",
    "    results.append(bge_result)\n",
    "    \n",
    "    # 3. E5-Large í…ŒìŠ¤íŠ¸\n",
    "    e5_result = test_e5_large_embedding(documents, test_queries)\n",
    "    results.append(e5_result)\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¹„êµ\n",
    "    print_comparison_summary(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_single_model(documents: List[Document], model_choice: str) -> Dict:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì œì•½ì´ ì‹¬í•  ë•Œ)\n",
    "    \n",
    "    Args:\n",
    "        documents: HR ë¬¸ì„œ ì²­í¬ë“¤\n",
    "        model_choice: 'openai', 'bge', 'e5' ì¤‘ ì„ íƒ\n",
    "    \"\"\"\n",
    "    test_queries = get_hr_test_queries()\n",
    "    \n",
    "    if model_choice.lower() == 'openai':\n",
    "        return test_openai_embedding(documents, test_queries)\n",
    "    elif model_choice.lower() == 'bge':\n",
    "        return test_bge_m3_embedding(documents, test_queries)\n",
    "    elif model_choice.lower() == 'e5':\n",
    "        return test_e5_large_embedding(documents, test_queries)\n",
    "    else:\n",
    "        raise ValueError(\"model_choiceëŠ” 'openai', 'bge', 'e5' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "def print_comparison_summary(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ìš”ì•½ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìš”ì•½\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§\n",
    "    valid_results = [r for r in results if \"error\" not in r]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # ì„±ëŠ¥ ë°ì´í„° ì¤€ë¹„\n",
    "    performance_data = []\n",
    "    for result in valid_results:\n",
    "        performance_data.append({\n",
    "            \"ëª¨ë¸\": result[\"model_name\"],\n",
    "            \"ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\": f\"{result.get('creation_time', 0):.2f}ì´ˆ\",\n",
    "            \"í‰ê·  ê²€ìƒ‰ ì‹œê°„\": f\"{result['avg_time']:.3f}ì´ˆ\",\n",
    "            \"ì´ ê²€ìƒ‰ ì‹œê°„\": f\"{result['total_time']:.3f}ì´ˆ\"\n",
    "        })\n",
    "    \n",
    "    # ê²€ìƒ‰ ì†ë„ìˆœ ì •ë ¬\n",
    "    performance_data.sort(key=lambda x: float(x[\"í‰ê·  ê²€ìƒ‰ ì‹œê°„\"].replace(\"ì´ˆ\", \"\")))\n",
    "    \n",
    "    print(\"\\nğŸ“Š ì„±ëŠ¥ ìˆœìœ„ (ë¹ ë¥¸ ìˆœ):\")\n",
    "    for i, data in enumerate(performance_data, 1):\n",
    "        print(f\"  {i}. {data['ëª¨ë¸']}\")\n",
    "        print(f\"     ë²¡í„°ìŠ¤í† ì–´ ìƒì„±: {data['ë²¡í„°ìŠ¤í† ì–´ ìƒì„±']}\")\n",
    "        print(f\"     í‰ê·  ê²€ìƒ‰ ì‹œê°„: {data['í‰ê·  ê²€ìƒ‰ ì‹œê°„']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ’¡ MVP ê°œë°œ ê¶Œì¥ì‚¬í•­:\")\n",
    "    if performance_data:\n",
    "        fastest_model = performance_data[0][\"ëª¨ë¸\"]\n",
    "        print(f\"  â€¢ ê²€ìƒ‰ ì†ë„ 1ìœ„: {fastest_model}\")\n",
    "        print(f\"  â€¢ 10ì¼ MVP ê°œë°œìš© ì¶”ì²œ: {fastest_model}\")\n",
    "        print(f\"  â€¢ Pinecone ì—°ë™ ì‹œ ì´ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥\")\n",
    "    \n",
    "    # ì—ëŸ¬ ë°œìƒí•œ ëª¨ë¸ë“¤ í‘œì‹œ\n",
    "    error_results = [r for r in results if \"error\" in r]\n",
    "    if error_results:\n",
    "        print(\"\\nâš ï¸  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•œ ëª¨ë¸:\")\n",
    "        for result in error_results:\n",
    "            print(f\"  â€¢ {result['model_name']}: {result['error']}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤\n",
    "def quick_test_all(documents: List[Document]):\n",
    "    \"\"\"\n",
    "    ë¹ ë¥¸ ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì¶”ì²œ)\n",
    "    \"\"\"\n",
    "    return compare_all_models(documents)\n",
    "\n",
    "def quick_test_one(documents: List[Document], model: str):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    return test_single_model(documents, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ë„êµ¬\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"ì‚¬ìš©ë²•:\")\n",
    "    print(\"1. ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸: quick_test_all(documents)\")\n",
    "    print(\"2. ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸: quick_test_one(documents, 'openai')\")\n",
    "    print(\"3. ê°œë³„ í•¨ìˆ˜ í˜¸ì¶œ:\")\n",
    "    print(\"   - test_openai_embedding(documents, queries)\")\n",
    "    print(\"   - test_bge_m3_embedding(documents, queries)\")  \n",
    "    print(\"   - test_e5_large_embedding(documents, queries)\")\n",
    "    \n",
    "    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:\n",
    "    # documents = load_hr_policy_document(\"04_ë³µì§€ì •ì±…_v1.0.md\")\n",
    "    # results = quick_test_all(documents)\n",
    "    # ë˜ëŠ”\n",
    "    # result = quick_test_one(documents, 'openai')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
