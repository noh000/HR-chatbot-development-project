{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4c9dcc",
   "metadata": {},
   "source": [
    "# ë¡œë” í…ŒìŠ¤íŠ¸\n",
    "- 04_ë³µì§€ì •ì±…_v1.0.md ê¸°ì¤€  \n",
    "TextLoader: 0.0285ì´ˆ  \n",
    "UnstructuredMarkdownLoader: 18.1923ì´ˆ  \n",
    "ì„±ëŠ¥ ì°¨ì´: 637.5ë°°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "def performance_test(file_path: str):\n",
    "    # TextLoader í…ŒìŠ¤íŠ¸\n",
    "    start = time.time()\n",
    "    text_loader = TextLoader(file_path, encoding=\"UTF-8\")\n",
    "    text_docs = text_loader.load()\n",
    "    text_time = time.time() - start\n",
    "    print(f\"TextLoader: {text_time:.4f}ì´ˆ\")\n",
    "    \n",
    "    # UnstructuredMarkdownLoader í…ŒìŠ¤íŠ¸\n",
    "    start = time.time()\n",
    "    unstructured_loader = UnstructuredMarkdownLoader(\n",
    "        file_path, \n",
    "        mode=\"elements\",\n",
    "        strategy=\"fast\"\n",
    "    )\n",
    "    unstructured_docs = unstructured_loader.load()\n",
    "    unstructured_time = time.time() - start\n",
    "    print(f\"UnstructuredMarkdownLoader: {unstructured_time:.4f}ì´ˆ\")\n",
    "    \n",
    "    print(f\"ì„±ëŠ¥ ì°¨ì´: {unstructured_time/text_time:.1f}ë°°\")\n",
    "\n",
    "file_path = \"./04_ë³µì§€ì •ì±…_v1.0.md\"\n",
    "performance_test(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580611ac",
   "metadata": {},
   "source": [
    "# TextLoader + í—¤ë” ê¸°ë°˜ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93dcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_hr_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    HR ì •ì±… ë¬¸ì„œë¥¼ TextLoader + MarkdownHeaderTextSplitterë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë“¤\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TextLoaderë¡œ ë¬¸ì„œ ë¡œë“œ\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 2. Document ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ\n",
    "    document_text = documents[0].page_content\n",
    "    \n",
    "    # 3. HR ë¬¸ì„œ êµ¬ì¡°ì— ë§ëŠ” í—¤ë” ì •ì˜\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"ë¬¸ì„œì œëª©\"),          # # ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ\n",
    "        (\"##\", \"ì •ì±…ëŒ€ë¶„ë¥˜\"),       # ## 1. íœ´ê°€ ë° íœ´ì§ ì œë„\n",
    "        (\"###\", \"ì •ì±…ì„¸ë¶€í•­ëª©\"),    # ### 1.1 ì—°ì°¨íœ´ê°€\n",
    "        # (\"####\", \"ì„¸ë¶€ì ˆì°¨\"),       # #### **ì‹ ì²­ ì ˆì°¨**\n",
    "    ]\n",
    "    \n",
    "    # 4. MarkdownHeaderTextSplitterë¡œ êµ¬ì¡°ì  ë¶„í• \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # í—¤ë” ì •ë³´ ìœ ì§€ (ì»¨í…ìŠ¤íŠ¸ì— ì¤‘ìš”)\n",
    "    )\n",
    "    \n",
    "    # 5. ë¬¸ìì—´ì„ split_textì— ì „ë‹¬ (Documentê°€ ì•„ë‹Œ str)\n",
    "    md_header_splits = markdown_splitter.split_text(document_text)\n",
    "    \n",
    "    # 6. ê¸´ ì„¹ì…˜ì„ ìœ„í•œ ì¶”ê°€ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # HR Q&Aì— ì í•©í•œ í¬ê¸°\n",
    "        chunk_overlap=200,      # ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„ë©\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # 7. ìµœì¢… ë¶„í•  ì ìš©\n",
    "    final_splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return final_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38200590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡œë”© ì„±ê³µ! ì´ 15ê°œ ì²­í¬ë¡œ ë¶„í• ë¨\n",
      "====================================================================================================\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ'}\n",
      "ë‚´ìš©: # ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ\n",
      "ë²„ì „: v1.0\n",
      "ì‘ì„±ì¼: 2023-09-18\n",
      "íŒŒì¼ëª…: 04_ë³µì§€ì •ì±…_v1.0.md\n",
      "ë³¸ ë¬¸ì„œëŠ” ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì„ì§ì›ì„ ìœ„í•œ ë³µì§€ì œë„ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤.\n",
      "---...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '1. íœ´ê°€ ë° íœ´ì§ ì œë„', 'ì •ì±…ì„¸ë¶€í•­ëª©': '1.1 ì—°ì°¨íœ´ê°€'}\n",
      "ë‚´ìš©: ## 1. íœ´ê°€ ë° íœ´ì§ ì œë„  \n",
      "### 1.1 ì—°ì°¨íœ´ê°€\n",
      "- ì—°ì°¨íœ´ê°€ëŠ” ê¸°ë³¸ 15ì¼ì…ë‹ˆë‹¤.\n",
      "- ê¸°ì¡´ ê·¼ì†ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ë§¤ë…„ 1ì›” 1ì¼ ë¶€ì—¬ë©ë‹ˆë‹¤. (ë‹¨, ì „ë…„ë„ ì¶œê·¼ìœ¨ì´ 80% ì´ìƒì´ì–´ì•¼ ë°œìƒ)\n",
      "- ë°œìƒí•œ ì—°ì°¨ëŠ” í•´ë‹¹ë…„ë„ 12ì›” 31ì¼ê¹Œì§€ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ë¯¸ì‚¬ìš© ì—°ì°¨ëŠ” ì´ì›”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. ì „ìê²°ì¬(íŒ€ì¥) ìŠ¹ì¸ í›„ íœ´ê°€ í™•ì •\n",
      "...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '1. íœ´ê°€ ë° íœ´ì§ ì œë„', 'ì •ì±…ì„¸ë¶€í•­ëª©': '1.2 ë³‘ê°€'}\n",
      "ë‚´ìš©: ### 1.2 ë³‘ê°€\n",
      "- ì—°ê°„ 5ì¼ ìœ ê¸‰ ë³‘ê°€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (5ì¼ ì´ˆê³¼ ì‹œ, ë¬´ê¸‰)\n",
      "- ë³‘ê°€ì˜ ì—°ê°„ ëˆ„ì ì¼ì´ 2ì¼ ì´ˆê³¼ ì‹œ, ì˜ì‚¬ ì§„ë‹¨ì„œ ë˜ëŠ” ì†Œê²¬ì„œë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. ì „ìê²°ì¬(íŒ€ì¥) ìŠ¹ì¸ í›„ íœ´ê°€ í™•ì •\n",
      "2. ì „ìê²°ì¬ ìŠ¹ì¸ ì‹œ ìë™ìœ¼ë¡œ Google Calendarì™€ ì—°ë™ë˜ì–´ íŒ€ì›ë“¤ê³¼ ê³µìœ \n",
      "3. ì „ìê²°ì œ ì‹œìŠ¤í…œì—ì„œ ...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '1. íœ´ê°€ ë° íœ´ì§ ì œë„', 'ì •ì±…ì„¸ë¶€í•­ëª©': '1.3 ê°€ì¡±ëŒë´„íœ´ê°€'}\n",
      "ë‚´ìš©: ### 1.3 ê°€ì¡±ëŒë´„íœ´ê°€\n",
      "- ë²•ì • ê¸°ì¤€ì— ë”°ë¼ ì—° 10ì¼ ë¬´ê¸‰ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ìë…€, ë¶€ëª¨(ë°°ìš°ìì˜ ë¶€ëª¨ í¬í•¨), ë°°ìš°ìì˜ ì§ˆë³‘Â·ì‚¬ê³ Â·ë…¸ë ¹ ëŒë´„ ì‚¬ìœ ì— í•œí•©ë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. ì „ìê²°ì¬(íŒ€ì¥) ìŠ¹ì¸ í›„ íœ´ê°€ í™•ì •\n",
      "2. ì „ìê²°ì¬ ìŠ¹ì¸ ì‹œ ìë™ìœ¼ë¡œ Google Calendarì™€ ì—°ë™ë˜ì–´ íŒ€ì›ë“¤ê³¼ ê³µìœ \n",
      "3. ì „ìê²°ì œ ì‹œìŠ¤í…œì—ì„œ ê°€...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '2. ë³µì§€í¬ì¸íŠ¸ ë° ì§€ì›ê¸ˆ', 'ì •ì±…ì„¸ë¶€í•­ëª©': '2.1 ë³µì§€í¬ì¸íŠ¸'}\n",
      "ë‚´ìš©: ## 2. ë³µì§€í¬ì¸íŠ¸ ë° ì§€ì›ê¸ˆ  \n",
      "### 2.1 ë³µì§€í¬ì¸íŠ¸\n",
      "- ì—°ê°„ 300ë§Œì›ì˜ ë³µì§€í¬ì¸íŠ¸ê°€ ì§€ê¸‰ë©ë‹ˆë‹¤.\n",
      "- ë‹¹í•´ë…„ë„ ì…ì‚¬ì(1ë…„ ë¯¸ë§Œ ê·¼ì†ì)ëŠ” ê·¼ë¬´ ê°œì›” ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì›”í•  ê³„ì‚°í•˜ì—¬ ë‹¹í•´ë¶„ ë³µì§€í¬ì¸íŠ¸ê°€ ì§€ê¸‰ë©ë‹ˆë‹¤. (ì§€ê¸‰ì¼: ìˆ˜ìŠµê¸°ê°„ ì¢…ë£Œì¼)\n",
      "- AKë³µì§€ëª°(https://akfamily.com)ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "- í¬ì¸íŠ¸ëŠ” ë§¤ë…„ 1ì›”ì— ì¼ê´„ ì§€...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '2. ë³µì§€í¬ì¸íŠ¸ ë° ì§€ì›ê¸ˆ', 'ì •ì±…ì„¸ë¶€í•­ëª©': '2.2 êµìœ¡ë¹„ ì§€ì›'}\n",
      "ë‚´ìš©: ### 2.2 êµìœ¡ë¹„ ì§€ì›\n",
      "- ì—°ê°„ 50ë§Œì› í•œë„ë¡œ ìê¸°ê³„ë°œ êµìœ¡ë¹„ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "- ì–´í•™, ì§ë¬´ ê´€ë ¨ ê°•ì˜, ìê²©ì¦ ê³¼ì • ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "- ì‚¬ì „ ìŠ¹ì¸ ì—†ì´ëŠ” ì§€ì›ê¸ˆ ì •ì‚°ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. Slack #ask-hr ì±„ë„ì— êµìœ¡ë¹„ ì§€ì› ì—¬ë¶€ í™•ì¸\n",
      "2. ì „ìê²°ì¬(íŒ€ì¥ â†’ Cë ˆë²¨) ì‚¬ì „ ìŠ¹ì¸\n",
      "3. êµìœ¡ ìˆ˜ê°• í›„ Slack #ask...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '3. ì—…ë¬´ ì¥ë¹„ ì§€ì›', 'ì •ì±…ì„¸ë¶€í•­ëª©': '3.1 ê¸°ë³¸ ì¥ë¹„'}\n",
      "ë‚´ìš©: ## 3. ì—…ë¬´ ì¥ë¹„ ì§€ì›  \n",
      "### 3.1 ê¸°ë³¸ ì¥ë¹„\n",
      "- ì…ì‚¬ ì‹œ ë…¸íŠ¸ë¶ 1ëŒ€ë¥¼ ì§€ê¸‰í•©ë‹ˆë‹¤.\n",
      "- ëª¨ë‹ˆí„°ëŠ” ìµœëŒ€ 2ëŒ€ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '3. ì—…ë¬´ ì¥ë¹„ ì§€ì›', 'ì •ì±…ì„¸ë¶€í•­ëª©': '3.2 ê°œì¸ì¥ë¹„ ë³´ì¡°'}\n",
      "ë‚´ìš©: ### 3.2 ê°œì¸ì¥ë¹„ ë³´ì¡°\n",
      "- 200ë§Œì›/2ë…„ í•œë„ë¡œ ê°œì¸ ì—…ë¬´ìš© ì¥ë¹„ êµ¬ì…ì„ ê¸°ë³¸ ì¥ë¹„ì™€ ë³„ë„ë¡œ ì¶”ê°€ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "- ì…ì‚¬ì¼ ê¸°ì¤€ìœ¼ë¡œ 2ë…„ë§ˆë‹¤ ê°±ì‹ ë©ë‹ˆë‹¤.\n",
      "- ë‹¹í•´ë…„ë„ ì…ì‚¬ì(1ë…„ ë¯¸ë§Œ ê·¼ì†ì)ëŠ” ìˆ˜ìŠµê¸°ê°„(3ê°œì›”) ì¢…ë£Œ í›„ ì‹ ì²­ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "- ì‹ ì²­ ê°€ëŠ¥ ì¥ë¹„: í‚¤ë³´ë“œ, ë§ˆìš°ìŠ¤, íƒœë¸”ë¦¿, í—¤ë“œì…‹\n",
      "- ê·¸ ì™¸ ì¥ë¹„ëŠ” ì´ë¬´íŒ€ì˜ ì‚¬ì „ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "####...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '4. ì¢…í•© ê±´ê°•ê²€ì§„'}\n",
      "ë‚´ìš©: ## 4. ì¢…í•© ê±´ê°•ê²€ì§„\n",
      "- ì—° 1íšŒ ì¢…í•© ê±´ê°•ê²€ì§„ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "- ì§€ì› ë²”ìœ„: ë³¸ì¸Â·ë°°ìš°ì(ì—° 1íšŒ), ë¶€ëª¨(ë°°ìš°ìì˜ ë¶€ëª¨ í¬í•¨, ì…ì‚¬ë…„ë„ ê¸°ì¤€ 2ë…„ì— 1íšŒ)\n",
      "- ë‹¹í•´ë…„ë„ ì…ì‚¬ì(1ë…„ ë¯¸ë§Œ ê·¼ì†ì)ëŠ” ë‹¤ìŒí•´ë¶€í„° ì§€ì› ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "ì˜ˆì‹œ: 2023ë…„ ì…ì‚¬ ì‹œ, 2024ë…„ë¶€í„° ì¢…í•© ê±´ê°•ê²€ì§„ ì´ìš© ê°€ëŠ¥\n",
      "- ì§€ì • ë³‘ì›(í•œêµ­ê±´ê°•ê´€ë¦¬í˜‘íšŒ)ì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "- ...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '5. ì¹´í˜/ìŠ¤ë‚µë°”'}\n",
      "ë‚´ìš©: ## 5. ì¹´í˜/ìŠ¤ë‚µë°”\n",
      "- ì‚¬ë¬´ì‹¤ ë‚´ ìŠ¤ë‚µë°”ì—ì„œ ë¬´ë£Œ ê°„ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- ì»¤í”¼ë¨¸ì‹ , ì°¨ë¥˜, ìŒë£Œ, ê³¼ì, ì»µë¼ë©´ ë“±ì„ ìƒì‹œ êµ¬ë¹„í•©ë‹ˆë‹¤.\n",
      "- ì¬ê³  ë¶€ì¡± ì‹œ Slack #ask-ga ì±„ë„ì„ í†µí•´ ìš”ì²­ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "---...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '6. ì‚¬ë‚´ ë™ì•„ë¦¬ í™œë™ ì§€ì›'}\n",
      "ë‚´ìš©: ## 6. ì‚¬ë‚´ ë™ì•„ë¦¬ í™œë™ ì§€ì›\n",
      "- ë™ì•„ë¦¬ í™œë™ì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì—°ê°„ ì˜ˆì‚°ì´ ì§€ê¸‰ë©ë‹ˆë‹¤. (2ë§Œì›/1ì¸/1ê°œì›”/1ê³³ í•œë„)\n",
      "ì˜ˆì‹œ: ë™ì•„ë¦¬ ì¸ì›ì´ 5ëª…ì¼ ê²½ìš°, ì›” 10ë§Œì› ì§€ì›\n",
      "- ì¸ë‹¹ ìµœëŒ€ 2ê³³ê¹Œì§€ í™œë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ìš´ë™, ê²Œì„, ë…ì„œ, ìŒì•… ë“± ë‹¤ì–‘í•œ í™œë™ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. Slack #ask-ga ì±„ë„ì—ì„œ ë™ì•„ë¦¬ ...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '7. ì„ì‹ Â·ì¶œì‚°Â·ìœ¡ì•„ ì§€ì›', 'ì •ì±…ì„¸ë¶€í•­ëª©': '7.1 ìœ¡ì•„ ì§€ì›ê¸ˆ'}\n",
      "ë‚´ìš©: ## 7. ì„ì‹ Â·ì¶œì‚°Â·ìœ¡ì•„ ì§€ì›  \n",
      "### 7.1 ìœ¡ì•„ ì§€ì›ê¸ˆ\n",
      "- ë§Œ 8ì„¸ ì´í•˜ ìë…€ 1ì¸ë‹¹ ì›” 20ë§Œì›ì˜ ìœ¡ì•„ ì§€ì›ê¸ˆì„ ì§€ê¸‰í•©ë‹ˆë‹¤. (ìµœëŒ€ 40ë§Œì›)\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. ì „ìê²°ì œ ì‹œìŠ¤í…œì—ì„œ ìœ¡ì•„ ì§€ì›ê¸ˆ ì‹ ì²­ì„œ ì‘ì„± ë° ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ ì œì¶œ\n",
      "2. ì´ë¬´íŒ€ ìŠ¹ì¸ í›„ ê¸‰ì—¬ì— ë°˜ì˜\n",
      "3. ë‹¤ìŒë‹¬ ê¸‰ì—¬ë‚ ë¶€í„° ì§€ê¸‰ (ë‹¹ì›”ë¶„ ì†Œê¸‰ ì§€ê¸‰)\n",
      "ì˜ˆì‹œ: 2023ë…„ 9ì›”...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '7. ì„ì‹ Â·ì¶œì‚°Â·ìœ¡ì•„ ì§€ì›', 'ì •ì±…ì„¸ë¶€í•­ëª©': '7.2 íƒœì•„ ê²€ì§„ íœ´ê°€'}\n",
      "ë‚´ìš©: ### 7.2 íƒœì•„ ê²€ì§„ íœ´ê°€\n",
      "- ì„ì‹  ì§ì›ì€ íƒœì•„ ê²€ì§„ ì‹œ ìœ ê¸‰ ë°˜ì°¨ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- (~28ì£¼ê¹Œì§€) 4ì£¼ë§ˆë‹¤ 1íšŒ, (29-36ì£¼) 2ì£¼ë§ˆë‹¤ 1íšŒ, (37ì£¼ ì´í›„) 1ì£¼ë§ˆë‹¤ 1íšŒ...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': '7. ì„ì‹ Â·ì¶œì‚°Â·ìœ¡ì•„ ì§€ì›', 'ì •ì±…ì„¸ë¶€í•­ëª©': '7.3 ë‚œì„ íœ´ê°€'}\n",
      "ë‚´ìš©: ### 7.3 ë‚œì„ íœ´ê°€\n",
      "- ë‚œì„ ì¹˜ë£Œ ëª©ì ì˜ ìœ ê¸‰ íœ´ê°€ë¥¼ ì—° 3ì¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "#### **ì‹ ì²­ ì ˆì°¨**\n",
      "1. ì „ìê²°ì œ ì‹œìŠ¤í…œì—ì„œ íœ´ê°€ ì‹ ì²­ì„œ ì‘ì„± ë° ë³‘ì› ì˜ˆì•½ì¦ ì²¨ë¶€\n",
      "2. ì „ìê²°ì¬(íŒ€ì¥) ìŠ¹ì¸ í›„ íœ´ê°€ í™•ì •\n",
      "---...\n",
      "\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'ë¬¸ì„œì œëª©': 'ê°€ì´ë‹¤ í”Œë ˆì´ ìŠ¤íŠœë””ì˜¤(GPS) ì§ì› ë³µì§€ì œë„ ì¢…í•© ì•ˆë‚´ì„œ', 'ì •ì±…ëŒ€ë¶„ë¥˜': 'ë¶€ì¹™'}\n",
      "ë‚´ìš©: ## ë¶€ì¹™\n",
      "- ë³¸ ì•ˆë‚´ì„œëŠ” 2023ë…„ 9ì›” ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "- íšŒì‚¬ ê²½ì˜ ë°©ì¹¨ì— ë”°ë¼ ì œë„ëŠ” ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë©°, ë³€ê²½ ì‹œ Confluence ê³µì§€ì‚¬í•­ì— ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤....\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë³µì§€ì •ì±… ì ìš©\n",
    "file_path = \"./04_ë³µì§€ì •ì±…_v1.0.md\"\n",
    "splits = load_hr_document(file_path)\n",
    "\n",
    "len_splits = len(splits)\n",
    "print(f\"âœ… ë¡œë”© ì„±ê³µ! ì´ {len_splits}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨\")\n",
    "print(\"=\"*100)\n",
    "for i in range(len_splits):\n",
    "    print(f\"ë©”íƒ€ë°ì´í„°: {splits[i].metadata}\")\n",
    "    print(f\"ë‚´ìš©: {splits[i].page_content[:200]}...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677f4b8",
   "metadata": {},
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ë¹„êµ\n",
    "- OpenAI text-embedding-3-small\n",
    "- BAAI/bge-m3\n",
    "- intfloat/multilingual-e5-large-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "# %pip install langchain langchain-openai langchain-community\n",
    "# %pip install openai python-dotenv\n",
    "# %pip install faiss-cpu\n",
    "\n",
    "# í° ëª¨ë¸ ì„¤ì¹˜ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€\n",
    "# %pip install sentence-transformers transformers\n",
    "# %pip install --no-cache-dir sentence-transformers\n",
    "# %pip install --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d847ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì¹˜ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    import openai\n",
    "    import sentence_transformers\n",
    "    import transformers\n",
    "    import faiss\n",
    "    import torch\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "    print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "def get_hr_test_queries() -> List[str]:\n",
    "    \"\"\"\n",
    "    HR ë¬¸ì„œ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    return [\n",
    "        \"ì—°ì°¨íœ´ê°€ëŠ” ëª‡ ì¼ì¸ê°€ìš”?\",\n",
    "        \"ë³µì§€í¬ì¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?\",\n",
    "        \"êµìœ¡ë¹„ ì§€ì› ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "        \"ìœ¡ì•„ ì§€ì›ê¸ˆì€ ì–¼ë§ˆë‚˜ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "        \"ê±´ê°•ê²€ì§„ì€ ì–¸ì œë¶€í„° ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "        \"ì¥ê¸° ê·¼ì†ì ê°€ì‚°ì¼ ê·œì •\",\n",
    "        \"ë³‘ê°€ ì‚¬ìš© ì‹œ í•„ìš”í•œ ì„œë¥˜\",\n",
    "        \"ë™ì•„ë¦¬ í™œë™ë¹„ ì§€ì› í•œë„\"\n",
    "    ]\n",
    "\n",
    "def test_search_performance(vector_store: FAISS, model_name: str, test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ì˜ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ” {model_name} ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_results = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nì§ˆë¬¸ {i}: {query}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ê²°ê³¼)\n",
    "        search_results = vector_store.similarity_search(query, k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        total_time += search_time\n",
    "        \n",
    "        print(f\"â±ï¸  ê²€ìƒ‰ ì‹œê°„: {search_time:.3f}ì´ˆ\")\n",
    "        print(\"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "        \n",
    "        query_result = {\n",
    "            \"query\": query,\n",
    "            \"search_time\": search_time,\n",
    "            \"results\": []\n",
    "        }\n",
    "        \n",
    "        for j, doc in enumerate(search_results, 1):\n",
    "            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)\n",
    "            preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "            print(f\"  {j}. {preview}...\")\n",
    "            \n",
    "            query_result[\"results\"].append({\n",
    "                \"rank\": j,\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        \n",
    "        model_results.append(query_result)\n",
    "    \n",
    "    result = {\n",
    "        \"model_name\": model_name,\n",
    "        \"total_time\": total_time,\n",
    "        \"avg_time\": total_time / len(test_queries),\n",
    "        \"queries\": model_results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} ì „ì²´ ì„±ëŠ¥:\")\n",
    "    print(f\"  ì´ ê²€ìƒ‰ ì‹œê°„: {total_time:.3f}ì´ˆ\")\n",
    "    print(f\"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {total_time/len(test_queries):.3f}ì´ˆ\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_openai_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    OpenAI text-embedding-3-small í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"OpenAI text-embedding-3-small\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        print(\"âœ… OpenAI ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def test_bge_m3_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    BAAI/bge-m3 í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-m3\",\n",
    "            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš©ì‹œ 'cuda'ë¡œ ë³€ê²½\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(\"âœ… BGE-M3 ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def test_e5_large_embedding(documents: List[Document], test_queries: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    intfloat/multilingual-e5-large-instruct í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model_name = \"multilingual-e5-large-instruct\"\n",
    "    print(f\"\\nğŸš€ {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš©ì‹œ 'cuda'ë¡œ ë³€ê²½\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(\"âœ… E5-Large ì„ë² ë”© ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        print(\"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...\")\n",
    "        start_time = time.time()\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({creation_time:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "        result = test_search_performance(vector_store, model_name, test_queries)\n",
    "        result[\"creation_time\"] = creation_time\n",
    "        \n",
    "        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del vector_store, embeddings\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ {model_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"model_name\": model_name, \"error\": str(e)}\n",
    "\n",
    "def compare_all_models(documents: List[Document]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ ë¹„êµ\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ ì„ë² ë”© ëª¨ë¸ ìˆœì°¨ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_queries = get_hr_test_queries()\n",
    "    results = []\n",
    "    \n",
    "    # ê° ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    print(f\"\\nğŸ“‹ {len(test_queries)}ê°œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    \n",
    "    # 1. OpenAI í…ŒìŠ¤íŠ¸\n",
    "    openai_result = test_openai_embedding(documents, test_queries)\n",
    "    results.append(openai_result)\n",
    "    \n",
    "    # 2. BGE-M3 í…ŒìŠ¤íŠ¸  \n",
    "    bge_result = test_bge_m3_embedding(documents, test_queries)\n",
    "    results.append(bge_result)\n",
    "    \n",
    "    # 3. E5-Large í…ŒìŠ¤íŠ¸\n",
    "    e5_result = test_e5_large_embedding(documents, test_queries)\n",
    "    results.append(e5_result)\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¹„êµ\n",
    "    print_comparison_summary(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_single_model(documents: List[Document], model_choice: str) -> Dict:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì œì•½ì´ ì‹¬í•  ë•Œ)\n",
    "    \n",
    "    Args:\n",
    "        documents: HR ë¬¸ì„œ ì²­í¬ë“¤\n",
    "        model_choice: 'openai', 'bge', 'e5' ì¤‘ ì„ íƒ\n",
    "    \"\"\"\n",
    "    test_queries = get_hr_test_queries()\n",
    "    \n",
    "    if model_choice.lower() == 'openai':\n",
    "        return test_openai_embedding(documents, test_queries)\n",
    "    elif model_choice.lower() == 'bge':\n",
    "        return test_bge_m3_embedding(documents, test_queries)\n",
    "    elif model_choice.lower() == 'e5':\n",
    "        return test_e5_large_embedding(documents, test_queries)\n",
    "    else:\n",
    "        raise ValueError(\"model_choiceëŠ” 'openai', 'bge', 'e5' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "def print_comparison_summary(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ìš”ì•½ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìš”ì•½\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§\n",
    "    valid_results = [r for r in results if \"error\" not in r]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # ì„±ëŠ¥ ë°ì´í„° ì¤€ë¹„\n",
    "    performance_data = []\n",
    "    for result in valid_results:\n",
    "        performance_data.append({\n",
    "            \"ëª¨ë¸\": result[\"model_name\"],\n",
    "            \"ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\": f\"{result.get('creation_time', 0):.2f}ì´ˆ\",\n",
    "            \"í‰ê·  ê²€ìƒ‰ ì‹œê°„\": f\"{result['avg_time']:.3f}ì´ˆ\",\n",
    "            \"ì´ ê²€ìƒ‰ ì‹œê°„\": f\"{result['total_time']:.3f}ì´ˆ\"\n",
    "        })\n",
    "    \n",
    "    # ê²€ìƒ‰ ì†ë„ìˆœ ì •ë ¬\n",
    "    performance_data.sort(key=lambda x: float(x[\"í‰ê·  ê²€ìƒ‰ ì‹œê°„\"].replace(\"ì´ˆ\", \"\")))\n",
    "    \n",
    "    print(\"\\nğŸ“Š ì„±ëŠ¥ ìˆœìœ„ (ë¹ ë¥¸ ìˆœ):\")\n",
    "    for i, data in enumerate(performance_data, 1):\n",
    "        print(f\"  {i}. {data['ëª¨ë¸']}\")\n",
    "        print(f\"     ë²¡í„°ìŠ¤í† ì–´ ìƒì„±: {data['ë²¡í„°ìŠ¤í† ì–´ ìƒì„±']}\")\n",
    "        print(f\"     í‰ê·  ê²€ìƒ‰ ì‹œê°„: {data['í‰ê·  ê²€ìƒ‰ ì‹œê°„']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ’¡ MVP ê°œë°œ ê¶Œì¥ì‚¬í•­:\")\n",
    "    if performance_data:\n",
    "        fastest_model = performance_data[0][\"ëª¨ë¸\"]\n",
    "        print(f\"  â€¢ ê²€ìƒ‰ ì†ë„ 1ìœ„: {fastest_model}\")\n",
    "        print(f\"  â€¢ 10ì¼ MVP ê°œë°œìš© ì¶”ì²œ: {fastest_model}\")\n",
    "        print(f\"  â€¢ Pinecone ì—°ë™ ì‹œ ì´ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥\")\n",
    "    \n",
    "    # ì—ëŸ¬ ë°œìƒí•œ ëª¨ë¸ë“¤ í‘œì‹œ\n",
    "    error_results = [r for r in results if \"error\" in r]\n",
    "    if error_results:\n",
    "        print(\"\\nâš ï¸  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•œ ëª¨ë¸:\")\n",
    "        for result in error_results:\n",
    "            print(f\"  â€¢ {result['model_name']}: {result['error']}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤\n",
    "def quick_test_all(documents: List[Document]):\n",
    "    \"\"\"\n",
    "    ë¹ ë¥¸ ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì¶”ì²œ)\n",
    "    \"\"\"\n",
    "    return compare_all_models(documents)\n",
    "\n",
    "def quick_test_one(documents: List[Document], model: str):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    return test_single_model(documents, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ë„êµ¬\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"ì‚¬ìš©ë²•:\")\n",
    "    print(\"1. ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸: quick_test_all(documents)\")\n",
    "    print(\"2. ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸: quick_test_one(documents, 'openai')\")\n",
    "    print(\"3. ê°œë³„ í•¨ìˆ˜ í˜¸ì¶œ:\")\n",
    "    print(\"   - test_openai_embedding(documents, queries)\")\n",
    "    print(\"   - test_bge_m3_embedding(documents, queries)\")  \n",
    "    print(\"   - test_e5_large_embedding(documents, queries)\")\n",
    "    \n",
    "    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:\n",
    "    # documents = load_hr_policy_document(\"04_ë³µì§€ì •ì±…_v1.0.md\")\n",
    "    # results = quick_test_all(documents)\n",
    "    # ë˜ëŠ”\n",
    "    # result = quick_test_one(documents, 'openai')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
